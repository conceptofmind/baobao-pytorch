import torch
from torch import nn
from torch.nn import functional as F
from functools import partial
from einops import rearrange
from flash_cosine_sim_attention import flash_cosine_sim_attention

# normalization

class RMSNorm(nn.Module):
    def __init__(self, dim, eps = 1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, dim = -1, keepdim = True) * self.scale
        return x / norm.clamp(min = self.eps) * self.g


class GEGLU(nn.Module):
    def forward(self, x):
        x, gate = x.chunk(2, dim=-1)
        return x * F.gelu(gate)

class FeedForward(nn.Module):
    def __init__(self, dim, ff_mult=4, dropout=0.):
        super().__init__()
        ff_inner_dim = int(dim * ff_mult)
        self.net = nn.Sequential(
            nn.Linear(dim, ff_inner_dim * 2, bias = False),
            GEGLU(),
            nn.Dropout(dropout),
            nn.Linear(ff_inner_dim, dim, bias = False)
        )
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(
        self,
        dim,
        dim_head = 64,
        heads = 8,
        scale = 8,
        **kwargs
    ):
        super().__init__()
        inner_dim = dim_head * heads
        self.scale = scale
        self.heads = heads

        self.attn_fn = partial(flash_cosine_sim_attention, **kwargs)

        self.to_q = nn.Linear(dim, inner_dim, bias = False)
        self.to_k = nn.Linear(dim, inner_dim, bias = False)
        self.to_v = nn.Linear(dim, inner_dim, bias = False)
        self.to_out = nn.Linear(inner_dim, dim, bias = False)

    def forward(self, x):
        h, scale = self.heads, self.scale

        q, k, v = self.to_q(x), self.to_k(x), self.to_v(x)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))

        o = self.attn_fn(q, k, v, causal = True, scale = scale)

        o = rearrange(o, 'b h n d -> b n (h d)')
        return self.to_out(o)

class ParallelAttention(nn.Module):
    def __init__(
        self, 
        dim, 
        dim_head = 64, 
        heads = 8, 
        scale = 8,
        ff_mult=4,
        dropout=0., 
        **kwargs
    ):
        super().__init__()
        self.attention = Attention(dim, dim_head, heads, scale, **kwargs)
        self.ffn = FeedForward(dim, ff_mult, dropout)

    def forward(self, x):
        return self.attention(x) + self.ffn(x)

class Transformer(nn.Module):
    def __init__(
        self, 
        dim, 
        depth, 
        dim_head = 64, 
        heads = 8, 
        scale = 8,
        ff_mult=4,
        dropout=0., 
        **kwargs
    ):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                ParallelAttention(dim, dim_head, heads, scale, ff_mult, dropout, **kwargs),
                RMSNorm(dim)
            ]))

    def forward(self, x):
        for attention, norm in self.layers:
            x = attention(x) + x
            x = norm(x)
        return x

class BaoBao(nn.Module):
    def __init__(
        self,
        num_tokens, 
        dim,
        max_seq_len, 
        depth, 
        dim_head = 64, 
        heads = 8, 
        scale = 8,
        ff_mult=4,
        dropout=0.,
        alpha = 0.1, 
        **kwargs
    ):
        super().__init__()

        self.alpha = alpha

        self.token_emb = nn.Embedding(num_tokens, dim)
        self.pos_emb = nn.Embedding(max_seq_len, dim)
        self.transformer = Transformer(dim, depth, dim_head, heads, scale, ff_mult, dropout, **kwargs)
        self.to_logits = nn.Linear(dim, num_tokens, bias = False)

        self.token_emb.weight = self.to_logits.weight
        nn.init.normal_(self.token_emb.weight, std=0.02)
        nn.init.normal_(self.pos_emb.weight, std=0.02)

    def forward(self, x):

        x = self.token_emb(x) * self.alpha + self.token_emb(x).detach() * (1 - self.alpha)
        x += self.pos_emb(torch.arange(x.shape[1], device=x.device))
        x = self.transformer(x)
        return self.to_logits(x)


if __name__ == '__main__':

    model = BaoBao(
        num_tokens = 20000,
        dim = 256,
        max_seq_len = 512, 
        depth = 6,
    ).cuda()

    x = torch.randint(0, 20000, (1, 512)).cuda()

    print(model(x).shape)

    n_params_torch = sum(
        p.numel() for p in model.parameters() if p.requires_grad
    )

    print(f"Number of parameters in torch model: {n_params_torch}")